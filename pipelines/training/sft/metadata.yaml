---
name: sft_pipeline
stability: alpha
dependencies:
  kubeflow:
    - name: Pipelines
      version: '>=2.15.2'
    - name: Trainer
      version: '>=0.1.0'
  external_services:
    - name: HuggingFace Datasets
      version: ">=2.14.0"
    - name: Kubernetes
      version: ">=1.28.0"
    - name: Model Registry
      version: ">=0.3.4"
tags:
  - training
  - fine_tuning
  - sft
  - supervised_fine_tuning
  - llm
  - language_model
  - pipeline
lastVerified: 2026-01-09T00:00:00Z
links:
  documentation: https://github.com/kubeflow/trainer
readme_sections:
  quick_start: |
    For a minimal run of this pipeline:

    - **Required parameter**:
      - Set `phase_01_dataset_man_data_uri` to a supported dataset, for example:
        - `hf://LipengCS/Table-GPT:All`
      - All other inputs have reasonable defaults and can be left unchanged for a first run.
    - **Pipeline server**: Must be configured for your OpenShift AI project (see Prerequisites below).
    - **Model Registry**: Optional. Leave `phase_04_registry_man_address` empty to skip registration.
    - **Secrets**:
      - **Required**: `kubernetes-credentials` (provides `KUBERNETES_SERVER_URL` and `KUBERNETES_AUTH_TOKEN`) so the
        training step can submit a Training Hub SFT job.
      - **Optional but often needed**:
        - `hf-token` (`HF_TOKEN`) if the chosen HF dataset or model is gated/private.
        - `s3-secret` if you use an `s3://` dataset URI instead of HF/HTTP/PVC.
        - `oci-pull-secret-model-download` if you use an `oci://` base model from a private registry.
  prerequisites: |
    - **Configured pipeline server (required)**
      Before you can upload or run this pipeline, your OpenShift AI project must have a **pipeline server**
      configured. The pipeline server defines where pipeline **artifacts and run data are stored**
      (S3-compatible object storage) and is the endpoint that executes compiled pipeline YAML.
      Follow the Red Hat OpenShift AI documentation for configuring a pipeline server:
      - [Configuring a pipeline server in OpenShift AI](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html/working_with_ai_pipelines/managing-ai-pipelines_ai-pipelines#configuring-a-pipeline-server_ai-pipelines)
      Without this, you will not be able to import the compiled YAML or create pipeline runs.

    - **Model Registry (optional, but required if you want registration)**
      Stage 4 of this pipeline can **register the fine-tuned model** into a Model Registry instance.
      If you plan to use this step (that is, you set `phase_04_registry_man_address`), you must first create and
      configure a Model Registry in the **same project/namespace** as the pipeline server, and note its service
      address and port. See the Red Hat documentation for details:
      - [Creating a Model Registry](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html/managing_model_registries/creating-a-model-registry_managing-model-registries)
      The pipeline’s Model Registry component uses this endpoint to register a new model version with training and
      evaluation metadata attached.

    - **Kubernetes secrets (required and optional)**
      All secrets must be created in the **same namespace as the pipeline server and pipeline runs** (for example,
      your OpenShift AI project namespace). They are mounted into tasks as environment variables via
      `kfp.kubernetes.use_secret_as_env`.

      - **`s3-secret` (required for `s3://` datasets)**
        - **Used by**: Dataset download step when `dataset_uri` starts with `s3://`.
        - **Keys (in `data` or `stringData`)**:
          - `AWS_ACCESS_KEY_ID`
          - `AWS_SECRET_ACCESS_KEY`
        - These are injected as env vars into the dataset component. For `s3://` URIs, both must be set and
          non-empty; otherwise the component fails fast with a clear error.

      - **`hf-token` (optional, recommended for gated models/datasets)**
        - **Used by**: Dataset download, training, and evaluation steps.
        - **Key**:
          - `HF_TOKEN` – a valid Hugging Face access token.
        - The pipeline injects this into all main tasks as the `HF_TOKEN` environment variable. It is required for
          **gated/private** Hugging Face datasets or models; if absent, the pipeline can only access public,
          non-gated assets and will log a warning.

      - **`kubernetes-credentials` (required)**
        - **Used by**: Training step (`train_model` component) to talk to the Kubernetes API and submit Training Hub
          SFT jobs.
        - **Keys**:
          - `KUBERNETES_SERVER_URL` – API server URL (for example, `https://api.<cluster-domain>:6443`).
          - `KUBERNETES_AUTH_TOKEN` – Bearer token with permission to list TrainingHub runtimes and manage jobs.
        - These are mounted as env vars and the training component **requires both to be set and non-empty**; if they
          are missing or mismatched, the component raises a clear configuration error instead of silently falling back.

      - **`oci-pull-secret-model-download` (optional, required for private OCI base models)**
        - **Used by**: Training step when `phase_02_train_man_model` is an `oci://` reference.
        - **Key**:
          - `OCI_PULL_SECRET_MODEL_DOWNLOAD` – the contents of a Docker `config.json`, used by `skopeo` to
            authenticate to the registry.
        - If this secret is not provided and the registry requires authentication, the OCI model download will fail
          with an authorization error. A common starting point is the cluster-wide pull secret; a cluster administrator
          can extract it with:

          oc get secret pull-secret -n openshift-config \
            -o jsonpath='{.data.\.dockerconfigjson}' | base64 -d | jq .

          You can then trim this `config.json` down to just the registries needed for your model and store it as the
          value of `OCI_PULL_SECRET_MODEL_DOWNLOAD` in an `Opaque` secret named `oci-pull-secret-model-download`.
  troubleshooting: |-
    - **Kubernetes credentials / TrainingHub errors**
      - Symptom: Training step fails with errors like `Failed to list ClusterTrainingRuntimes` or HTTP `401 Unauthorized`.
      - Likely cause: `kubernetes-credentials` secret missing or misconfigured:
        - Ensure the secret exists in the pipeline namespace.
        - Ensure it has **both** `KUBERNETES_SERVER_URL` and `KUBERNETES_AUTH_TOKEN` keys set and non-empty.
      - The training component requires both env vars and will raise a clear configuration error if they are missing.

    - **S3 dataset failures**
      - Symptom: Dataset step fails with a `ValueError` mentioning `S3 credentials misconfigured` or
        `S3 credentials missing`.
      - Likely cause: `s3-secret` secret missing or only one of `AWS_ACCESS_KEY_ID` / `AWS_SECRET_ACCESS_KEY` is set.
      - Fix: Create/update `s3-secret` in the pipeline namespace with both keys populated; for `s3://` URIs both must
        be set.

    - **OCI model download failures**
      - Symptom: Training step logs show `skopeo copy failed` with `unauthorized` or `authentication required`.
      - Likely cause: `oci-pull-secret-model-download` secret is missing or does not contain a valid Docker
        `config.json` as `OCI_PULL_SECRET_MODEL_DOWNLOAD`.
      - Fix: Create the secret in the pipeline namespace with a trimmed `config.json` that includes credentials for
        the registries hosting your model image (see Prerequisites above for an example command to extract the
        cluster-wide pull secret as a starting point).

    - **Hugging Face gated datasets or models**
      - Symptom: Dataset or training step fails with 403/permission errors from Hugging Face, or logs warnings that
        `HF_TOKEN` is not set when using HF IDs (for example, gated models or datasets).
      - Likely cause: `hf-token` secret is missing or the token does not have access to the requested asset.
      - Fix: Create `hf-token` with a valid `HF_TOKEN` value in the pipeline namespace, and ensure the token has
        accepted the dataset/model license terms on the Hugging Face Hub.

    - **Chat-format validation errors**
      - Symptom: Dataset step fails with messages like “missing 'messages' or 'conversations' field” or invalid `role`.
      - Likely cause: Input data is not in the expected chat format (a list of messages/conversations with `role` and
        `content` fields, roles in `{system, user, assistant, function, tool}`).
      - Fix: Inspect the source dataset (JSON/JSONL or HF dataset) and adjust it to match the expected structure, or
        choose a dataset that already uses a compatible chat schema.

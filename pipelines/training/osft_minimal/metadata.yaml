---
name: osft_minimal_pipeline
stability: alpha
dependencies:
  kubeflow:
    - name: Pipelines
      version: '>=2.15.2'
    - name: Trainer
      version: '>=0.1.0'
  external_services:
    - name: HuggingFace Datasets
      version: ">=2.14.0"
    - name: Kubernetes
      version: ">=1.28.0"
    - name: Model Registry
      version: ">=0.3.4"
tags:
  - training
  - fine_tuning
  - osft
  - orthogonal_subspace
  - continual_learning
  - minimal
  - llm
  - language_model
  - pipeline
lastVerified: 2026-01-14T00:00:00Z
links:
  documentation: https://github.com/kubeflow/trainer
readme_sections:
  quick_start: |
    For a minimal run of this pipeline:

    - **Required parameter**:
      - Set `phase_01_dataset_man_data_uri` to a supported dataset, for example:
        - `hf://LipengCS/Table-GPT:All`
      - All other inputs have reasonable defaults and can be left unchanged for a first run.
    - **Pipeline server**: Must be configured for your OpenShift AI project (see Prerequisites below).
    - **Model Registry**: Optional. Leave `phase_04_registry_man_address` empty to skip registration.
    - **Secrets**:
      - **Required**: `kubernetes-credentials` (provides `KUBERNETES_SERVER_URL` and `KUBERNETES_AUTH_TOKEN`) so the
        training step can submit a Training Hub OSFT job.
      - **Optional but often needed**:
        - `hf-token` (`HF_TOKEN`) if the chosen HF dataset or model is gated/private.
        - `s3-secret` if you use an `s3://` dataset URI instead of HF/HTTP/PVC.
        - `oci-pull-secret-model-download` if you use an `oci://` base model from a private registry.
  prerequisites: |
    - **Configured pipeline server (required)**
      Before you can upload or run this pipeline, your OpenShift AI project must have a **pipeline server**
      configured. The pipeline server defines where pipeline **artifacts and run data are stored**
      (S3-compatible object storage) and is the endpoint that executes compiled pipeline YAML.
      Follow the Red Hat OpenShift AI documentation for configuring a pipeline server:
      - [Configuring a pipeline server in OpenShift AI](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html/working_with_ai_pipelines/managing-ai-pipelines_ai-pipelines#configuring-a-pipeline-server_ai-pipelines)
      Without this, you will not be able to import the compiled YAML or create pipeline runs.

    - **Model Registry (optional, but required if you want registration)**
      Stage 4 of this pipeline can **register the fine-tuned model** into a Model Registry instance.
      If you plan to use this step (that is, you set `phase_04_registry_man_address`), you must first create and
      configure a Model Registry in the **same project/namespace** as the pipeline server, and note its service
      address and port. See the Red Hat documentation for details:
      - [Creating a Model Registry](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html/managing_model_registries/creating-a-model-registry_managing-model-registries)
      The pipeline’s Model Registry component uses this endpoint to register a new model version with training and
      evaluation metadata attached.

    - **Kubernetes secrets (required and optional)**
      All secrets must be created in the **same namespace as the pipeline server and pipeline runs** (for example,
      your OpenShift AI project namespace). They are mounted into tasks as environment variables via
      `kfp.kubernetes.use_secret_as_env`.

      - **`s3-secret` (required for `s3://` datasets)**
        - **Used by**: Dataset download step when `dataset_uri` starts with `s3://`.
        - **Keys (in `data` or `stringData`)**:
          - `AWS_ACCESS_KEY_ID`
          - `AWS_SECRET_ACCESS_KEY`
        - These are injected as env vars into the dataset component. For `s3://` URIs, both must be set and
          non-empty; otherwise the component fails fast with a clear error.

      - **`hf-token` (optional, recommended for gated models/datasets)**
        - **Used by**: Dataset download, training, and evaluation steps.
        - **Key**:
          - `HF_TOKEN` – a valid Hugging Face access token.
        - The pipeline injects this into all main tasks as the `HF_TOKEN` environment variable. It is required for
          **gated/private** Hugging Face datasets or models; if absent, the pipeline can only access public,
          non-gated assets and will log a warning.

      - **`kubernetes-credentials` (required)**
        - **Used by**: Training step (`train_model` component) to talk to the Kubernetes API and submit Training Hub
          OSFT jobs.
        - **Keys**:
          - `KUBERNETES_SERVER_URL` – API server URL (for example, `https://api.<cluster-domain>:6443`).
          - `KUBERNETES_AUTH_TOKEN` – Bearer token with permission to list TrainingHub runtimes and manage jobs.
        - These are mounted as env vars and the training component **requires both to be set and non-empty**; if they
          are missing or mismatched, the component raises a clear configuration error instead of silently falling back.

      - **`oci-pull-secret-model-download` (optional, required for private OCI base models)**
        - **Used by**: Training step when `phase_02_train_man_train_model` is an `oci://` reference.
        - **Key**:
          - `OCI_PULL_SECRET_MODEL_DOWNLOAD` – the contents of a Docker `config.json`, used by `skopeo` to
            authenticate to the registry.
        - If this secret is not provided and the registry requires authentication, the OCI model download will fail
          with an authorization error. A common starting point is the cluster-wide pull secret; a cluster administrator
          can extract it with:

          oc get secret pull-secret -n openshift-config \
            -o jsonpath='{.data.\.dockerconfigjson}' | base64 -d | jq .

          You can then trim this `config.json` down to just the registries needed for your model and store it as the
          value of `OCI_PULL_SECRET_MODEL_DOWNLOAD` in an `Opaque` secret named `oci-pull-secret-model-download`.
